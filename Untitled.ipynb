{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6713895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_specific_part(html_file_path, tag_name, id_name=None):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    if id_name:\n",
    "        specific_part = soup.find(tag_name, id=id_name)\n",
    "    else:\n",
    "        specific_part = soup.find(tag_name)\n",
    "\n",
    "    return specific_part\n",
    "\n",
    "html_file_path = 'customer.html'  \n",
    "tag_name = 'Business Context and Application'  \n",
    "id_name = 'specific-id' \n",
    "\n",
    "specific_part = extract_specific_part(html_file_path, tag_name, id_name)\n",
    "print(specific_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04aa8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_soup_from_html(html_file_path):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "html_file_path = 'customer.html' \n",
    "soup = create_soup_from_html(html_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1a94eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element not found.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_specific_info(html_file_path, tag_name, id_name=None):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    if id_name:\n",
    "        specific_element = soup.find(tag_name, id=id_name)\n",
    "    else:\n",
    "        specific_element = soup.find(tag_name)\n",
    "    \n",
    "    return specific_element\n",
    "\n",
    "html_file_path = 'customer.html'  \n",
    "tag_name = 'h1'  \n",
    "id_name = 'specific-id'  \n",
    "\n",
    "specific_element = extract_specific_info(html_file_path, tag_name, id_name)\n",
    "if specific_element:\n",
    "    print(\"Found element:\")\n",
    "    print(specific_element)\n",
    "    print(\"Text content:\")\n",
    "    print(specific_element.text)\n",
    "else:\n",
    "    print(\"Element not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950c27a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element not found.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_specific_info(html_file_path, tag_name, id_name=None):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    if id_name:\n",
    "        specific_element = soup.find(tag_name, id=id_name)\n",
    "    else:\n",
    "        specific_element = soup.find(tag_name)\n",
    "    \n",
    "    return specific_element\n",
    "\n",
    "html_file_path = 'customer.html'  # Replace 'sample.html' with the path to your HTML file\n",
    "tag_name = 'div'  # Specify the tag name you want to extract\n",
    "id_name = 'none'  # Specify the id name (if any) of the tag you want to extract\n",
    "\n",
    "specific_element = extract_specific_info(html_file_path, tag_name, id_name)\n",
    "if specific_element:\n",
    "    print(\"Found element:\")\n",
    "    print(specific_element)\n",
    "    print(\"Text content:\")\n",
    "    print(specific_element.text)\n",
    "else:\n",
    "    print(\"Element not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13181d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No information found following the heading.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_heading_information(html_file_path, heading_text):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    heading_element = soup.find('strong', string=lambda text: heading_text in text)\n",
    "\n",
    "    if heading_element:\n",
    "        next_sibling = heading_element.find_next_sibling()\n",
    "        if next_sibling:\n",
    "            heading_information = next_sibling.text.strip()\n",
    "            return heading_information\n",
    "        else:\n",
    "            return \"No information found following the heading.\"\n",
    "    else:\n",
    "        return \"Heading not found.\"\n",
    "\n",
    "html_file_path = 'doc.html'  \n",
    "heading_text = 'Overview of the Data Product'\n",
    "heading_information = extract_heading_information(html_file_path, heading_text)\n",
    "\n",
    "print(heading_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e72744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interoperability:\n",
      "Reliability:\n",
      "Governance:\n",
      "Discoverability:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('doc.html', 'r') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "h1_headings = soup.find_all('h3')\n",
    "for heading in h1_headings:\n",
    "    print(heading.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34101a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer 360 V2 (Technical Documentation): h1\n",
      "Interoperability:: h3\n",
      "Reliability:: h3\n",
      "Governance:: h3\n",
      "Discoverability:: h3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_headings_as_words(html_file_path):\n",
    "    with open(html_file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    headings = {}\n",
    "\n",
    "    for heading_tag in ['h1', 'h2', 'h3']:\n",
    "        heading_elements = soup.find_all(heading_tag)\n",
    "        for heading in heading_elements:\n",
    "            heading_text = ' '.join(word.strip() for word in heading.get_text().split())\n",
    "            if heading_text:\n",
    "                headings[heading_text] = heading_tag\n",
    "\n",
    "    return headings\n",
    "\n",
    "def display_headings_as_words(headings):\n",
    "    for heading, tag in headings.items():\n",
    "        print(f\"{heading}: {tag}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html_file_path = \"doc.html\"\n",
    "    headings = extract_headings_as_words(html_file_path)\n",
    "    display_headings_as_words(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a279e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading 'Overview of the Data Product' not found in the HTML file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/3150343765.py:8: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  heading = soup.find('h2', text=heading_text)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_heading_text(html_file_path, heading_text):\n",
    "    with open(html_file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    heading = soup.find('h2', text=heading_text)\n",
    "\n",
    "    if heading:\n",
    "        return heading.get_text().strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html_file_path = \"doc.html\"\n",
    "    heading_text = \"Overview of the Data Product\"\n",
    "    heading_text_found = extract_heading_text(html_file_path, heading_text)\n",
    "\n",
    "    if heading_text_found:\n",
    "        print(f\"Text content of '{heading_text}':\\n{heading_text_found}\")\n",
    "    else:\n",
    "        print(f\"Heading '{heading_text}' not found in the HTML file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecdf5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Jupyter Notebook\n",
      "  </title>\n",
      "  <link href=\"/static/base/images/favicon.ico?v=50afa725b5de8b00030139d09b38620224d4e7dba47c07ef0e86d4643f30c9bfe6bb7e1a4a1c561aa32834480909a4b6fe7cd1e17f7159330b6b5914bf45a880\" id=\"favicon\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n",
      "   <link href=\"/static/components/jquery-ui/themes/smoothness/jquery-ui.min.css?v=fb45616eef2c454960f91fcd2a04efeda84cfacccf0c5d741ba2793dc1dbd6d3ab01aaae6485222945774c7d7a9a2e9fb87e0d8ef1ea96893aa6906147a371bb\" rel=\"stylesheet\" type=\"text/css\">\n",
      "    <link href=\"/static/components/jquery-typeahead/dist/jquery.typeahead.min.css?v=5edf53bf6bb9c3b1ddafd8594825a7e2ed621f19423e569c985162742f63911c09eba2c529f8fb47aebf27fafdfe287d563347f58c1126b278189a18871b6a9a\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "    <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "    <link href=\"/static/style/style.min.css?v=b092d0a2da5df36f2b073ddb4eafcd6c8094c4fa21b6dcd3f7185ce16c04ad66424083d785b81607a26b4b85b69a560574ada7db75262c886655f99d651c482e\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "    <link href=\"/static/auth/css/override.css?v=7d24c52c4d63c7ed3ca286c051f232acd6a5ee3ed2aa32f56bea22ef20a2e3d184a638aad5602a39e8d5ef9a66ed93fdbaf80cf979948a947bab117618bd6486\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "    <link href=\"/custom/custom.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "    <script charset=\"utf-8\" src=\"/static/components/es6-promise/promise.min.js?v=bea335d74136a63ae1b5130f5ac9a50c6256a5f435e6e09fef599491a84d834a8b0f011ca3eaaca3b4ab6a2da2d3e1191567a2f171e60da1d10e5b9d52f84184\" type=\"text/javascript\">\n",
      "    </script>\n",
      "    <script src=\"/static/components/react/react.production.min.js?v=9a0aaf84a316c8bedd6c2ff7d5b5e0a13f8f84ec02442346cba0b842c6c81a6bf6176e64f3675c2ebf357cb5bb048e0b527bd39377c95681d22468da3d5de735\" type=\"text/javascript\">\n",
      "    </script>\n",
      "    <script src=\"/static/components/react/react-dom.production.min.js?v=6fc58c1c4736868ff84f57bd8b85f2bdb985993a9392718f3b4af4bfa10fb4efba2b4ddd68644bd2a8daf0619a3844944c9c43f8528364a1aa6fc01ec1b8ae84\" type=\"text/javascript\">\n",
      "    </script>\n",
      "    <script src=\"/static/components/create-react-class/index.js?v=894ad57246e682b4cfbe7cd5e408dcd6b38d06af4de4f3425991e2676fdc2ef1732cbd19903104198878ae77de12a1996de3e7da3a467fb226bdda8f4618faec\" type=\"text/javascript\">\n",
      "    </script>\n",
      "    <script charset=\"utf-8\" src=\"/static/components/requirejs/require.js?v=d37b48bb2137faa0ab98157e240c084dd5b1b5e74911723aa1d1f04c928c2a03dedf922d049e4815f7e5a369faa2e6b6a1000aae958b7953b5cc60411154f593\" type=\"text/javascript\">\n",
      "    </script>\n",
      "    <script>\n",
      "     require.config({\n",
      "          \n",
      "          urlArgs: \"v=20240214125751\",\n",
      "          \n",
      "          baseUrl: '/static/',\n",
      "          paths: {\n",
      "            'auth/js/main': 'auth/js/main.min',\n",
      "            custom : '/custom',\n",
      "            nbextensions : '/nbextensions',\n",
      "            kernelspecs : '/kernelspecs',\n",
      "            underscore : 'components/underscore/underscore-min',\n",
      "            backbone : 'components/backbone/backbone-min',\n",
      "            jed: 'components/jed/jed',\n",
      "            jquery: 'components/jquery/jquery.min',\n",
      "            json: 'components/requirejs-plugins/src/json',\n",
      "            text: 'components/requirejs-text/text',\n",
      "            bootstrap: 'components/bootstrap/dist/js/bootstrap.min',\n",
      "            bootstraptour: 'components/bootstrap-tour/build/js/bootstrap-tour.min',\n",
      "            'jquery-ui': 'components/jquery-ui/jquery-ui.min',\n",
      "            moment: 'components/moment/min/moment-with-locales',\n",
      "            codemirror: 'components/codemirror',\n",
      "            termjs: 'components/xterm.js/xterm',\n",
      "            typeahead: 'components/jquery-typeahead/dist/jquery.typeahead.min',\n",
      "          },\n",
      "          map: { // for backward compatibility\n",
      "              \"*\": {\n",
      "                  \"jqueryui\": \"jquery-ui\",\n",
      "              }\n",
      "          },\n",
      "          shim: {\n",
      "            typeahead: {\n",
      "              deps: [\"jquery\"],\n",
      "              exports: \"typeahead\"\n",
      "            },\n",
      "            underscore: {\n",
      "              exports: '_'\n",
      "            },\n",
      "            backbone: {\n",
      "              deps: [\"underscore\", \"jquery\"],\n",
      "              exports: \"Backbone\"\n",
      "            },\n",
      "            bootstrap: {\n",
      "              deps: [\"jquery\"],\n",
      "              exports: \"bootstrap\"\n",
      "            },\n",
      "            bootstraptour: {\n",
      "              deps: [\"bootstrap\"],\n",
      "              exports: \"Tour\"\n",
      "            },\n",
      "            \"jquery-ui\": {\n",
      "              deps: [\"jquery\"],\n",
      "              exports: \"$\"\n",
      "            }\n",
      "          },\n",
      "          waitSeconds: 30,\n",
      "      });\n",
      "\n",
      "      require.config({\n",
      "          map: {\n",
      "              '*':{\n",
      "                'contents': 'services/contents',\n",
      "              }\n",
      "          }\n",
      "      });\n",
      "\n",
      "      // error-catching custom.js shim.\n",
      "      define(\"custom\", function (require, exports, module) {\n",
      "          try {\n",
      "              var custom = require('custom/custom');\n",
      "              console.debug('loaded custom.js');\n",
      "              return custom;\n",
      "          } catch (e) {\n",
      "              console.error(\"error loading custom.js\", e);\n",
      "              return {};\n",
      "          }\n",
      "      })\n",
      "\n",
      "    document.nbjs_translations = {\"domain\": \"nbjs\", \"locale_data\": {\"nbjs\": {\"\": {\"domain\": \"nbjs\"}}}};\n",
      "    document.documentElement.lang = navigator.language.toLowerCase();\n",
      "    </script>\n",
      "   </link>\n",
      "  </meta>\n",
      " </head>\n",
      " <body class=\"\" dir=\"ltr\">\n",
      "  <noscript>\n",
      "   <div id=\"noscript\">\n",
      "    Jupyter Notebook requires JavaScript.\n",
      "    <br/>\n",
      "    Please enable it to proceed.\n",
      "   </div>\n",
      "  </noscript>\n",
      "  <div aria-label=\"Top Menu\" id=\"header\" role=\"navigation\">\n",
      "   <div class=\"container\" id=\"header-container\">\n",
      "    <div class=\"nav navbar-brand\" id=\"ipython_notebook\">\n",
      "     <a href=\"/tree\" title=\"dashboard\">\n",
      "      <img alt=\"Jupyter Notebook\" src=\"/static/base/images/logo.png?v=a2a176ee3cee251ffddf5fa21fe8e43727a9e5f87a06f9c91ad7b776d9e9d3d5e0159c16cc188a3965e00375fb4bc336c16067c688f5040c0c2d4bfdb852a9e4\"/>\n",
      "     </a>\n",
      "    </div>\n",
      "   </div>\n",
      "   <div class=\"header-bar\">\n",
      "   </div>\n",
      "  </div>\n",
      "  <div id=\"site\">\n",
      "   <div class=\"container\" id=\"ipython-main-app\">\n",
      "    <div class=\"row\">\n",
      "     <div class=\"navbar col-sm-8\">\n",
      "      <div class=\"navbar-inner\">\n",
      "       <div class=\"container\">\n",
      "        <div class=\"center-nav\">\n",
      "         <form action=\"/login?next=%2Fview%2Fdoc.html\" class=\"navbar-form pull-left\" method=\"post\">\n",
      "          <input name=\"_xsrf\" type=\"hidden\" value=\"2|210dfdc8|303734380beb83376cc162670efc901f|1707980035\"/>\n",
      "          <label for=\"password_input\">\n",
      "           <strong>\n",
      "            Password or token:\n",
      "           </strong>\n",
      "          </label>\n",
      "          <input class=\"form-control\" id=\"password_input\" name=\"password\" type=\"password\"/>\n",
      "          <button class=\"btn btn-default\" id=\"login_submit\" type=\"submit\">\n",
      "           Log in\n",
      "          </button>\n",
      "         </form>\n",
      "        </div>\n",
      "       </div>\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "    <div class=\"col-sm-6 col-sm-offset-3 text-left rendered_html\">\n",
      "     <h3>\n",
      "      Token authentication is enabled\n",
      "     </h3>\n",
      "     <p>\n",
      "      If no password has been configured, you need to open the notebook\n",
      "        server with its login token in the URL, or paste it above.\n",
      "        This requirement will be lifted if you\n",
      "      <b>\n",
      "       <a href=\"https://jupyter-notebook.readthedocs.io/en/stable/public_server.html\">\n",
      "        enable a password\n",
      "       </a>\n",
      "      </b>\n",
      "      .\n",
      "     </p>\n",
      "     <p>\n",
      "      The command:\n",
      "      <pre>jupyter notebook list</pre>\n",
      "      will show you the URLs of running servers with their tokens,\n",
      "        which you can copy and paste into your browser. For example:\n",
      "     </p>\n",
      "     <pre>Currently running servers:\n",
      "http://localhost:8888/?token=c8de56fa... :: /Users/you/notebooks\n",
      "</pre>\n",
      "     <p>\n",
      "      or you can paste just the token value into the password field on this\n",
      "        page.\n",
      "     </p>\n",
      "     <p>\n",
      "      See\n",
      "      <b>\n",
      "       <a href=\"https://jupyter-notebook.readthedocs.io/en/stable/public_server.html\">\n",
      "        the documentation on how to enable a password\n",
      "       </a>\n",
      "      </b>\n",
      "      in place of token authentication,\n",
      "        if you would like to avoid dealing with random tokens.\n",
      "     </p>\n",
      "     <p>\n",
      "      Cookies are required for authenticated access to notebooks.\n",
      "     </p>\n",
      "     <h3>\n",
      "      Setup a Password\n",
      "     </h3>\n",
      "     <p>\n",
      "      You can also setup a password by entering your token and a new password\n",
      "        on the fields below:\n",
      "     </p>\n",
      "     <form action=\"/login?next=%2Fview%2Fdoc.html\" class=\"\" method=\"post\">\n",
      "      <input name=\"_xsrf\" type=\"hidden\" value=\"2|210dfdc8|303734380beb83376cc162670efc901f|1707980035\">\n",
      "       <div class=\"form-group\">\n",
      "        <label for=\"token_input\">\n",
      "         <h4>\n",
      "          Token\n",
      "         </h4>\n",
      "        </label>\n",
      "        <input class=\"form-control\" id=\"token_input\" name=\"password\" type=\"password\"/>\n",
      "       </div>\n",
      "       <div class=\"form-group\">\n",
      "        <label for=\"new_password_input\">\n",
      "         <h4>\n",
      "          New Password\n",
      "         </h4>\n",
      "        </label>\n",
      "        <input class=\"form-control\" id=\"new_password_input\" name=\"new_password\" required=\"\" type=\"password\"/>\n",
      "       </div>\n",
      "       <div class=\"form-group\">\n",
      "        <button class=\"btn btn-default\" id=\"login_new_pass_submit\" type=\"submit\">\n",
      "         Log in and set new password\n",
      "        </button>\n",
      "       </div>\n",
      "      </input>\n",
      "     </form>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      "  <script type=\"text/javascript\">\n",
      "   require([\"auth/js/main\"], function (auth) {\n",
      "    auth.login_main();\n",
      "  });\n",
      "  </script>\n",
      "  <script type=\"text/javascript\">\n",
      "   function _remove_token_from_url() {\n",
      "    if (window.location.search.length <= 1) {\n",
      "      return;\n",
      "    }\n",
      "    var search_parameters = window.location.search.slice(1).split('&');\n",
      "    for (var i = 0; i < search_parameters.length; i++) {\n",
      "      if (search_parameters[i].split('=')[0] === 'token') {\n",
      "        // remote token from search parameters\n",
      "        search_parameters.splice(i, 1);\n",
      "        var new_search = '';\n",
      "        if (search_parameters.length) {\n",
      "          new_search = '?' + search_parameters.join('&');\n",
      "        }\n",
      "        var new_url = window.location.origin + \n",
      "                      window.location.pathname + \n",
      "                      new_search + \n",
      "                      window.location.hash;\n",
      "        window.history.replaceState({}, \"\", new_url);\n",
      "        return;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  _remove_token_from_url();\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://localhost:8890/view/doc.html\"\n",
    "res = requests.get(url)\n",
    "htmlData = res.content\n",
    "parsedData = BeautifulSoup(htmlData, \"html.parser\")\n",
    "print(parsedData.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07adf172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content of 'Overview of the Data Product':\n",
      "Overview of the Data Product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/2148472098.py:8: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  heading = soup.find('strong', text=lambda t: heading_text.lower() in t.lower())\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_heading_text(html_file_path, heading_text):\n",
    "    with open(html_file_path, 'r') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    heading = soup.find('strong', text=lambda t: heading_text.lower() in t.lower())\n",
    "\n",
    "    if heading:\n",
    "        return heading.get_text().strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html_file_path = \"doc.html\"\n",
    "    heading_text = \"Overview of the Data Product\"\n",
    "    heading_text_found = extract_heading_text(html_file_path, heading_text)\n",
    "\n",
    "    if heading_text_found:\n",
    "        print(f\"Text content of '{heading_text}':\\n{heading_text_found}\")\n",
    "    else:\n",
    "        print(f\"Heading '{heading_text}' not found in the HTML file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0917bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: ['models', 'meaningful', 'presence', 'optimized', 'develop', 'computational', 'emphasis', 'deploy', 'implications', 'retain', 'essential', 'dataos', 'repeat', 'insights', 'approach', 'roi', 'connecting', 'employed', 'profound', 'its', 'optimizing', 'prescribed', 'page', 'incorporating', 'maintaining', 'questioning', 'identifies', 'standards', 'cohesive', 'role', 'capabilities', 'enhances', 'given', 'attributes', 'integral', 'sentiment', 'need', 'fostering', 'tasked', 'gain', 'pricing', 'delivery', 'drive', 'efficiently', 'operations', 'vary', 'type', 'audit', 'app', 'recommendations', 'indicative', 'lenses', 'securely', 'near', 'primary', 'xlarge', 'commitment', 'relationships', 'precise', 'valuable', 'initially', 'complex', 'ecosystem', 'individuals', 'language', 'multi', 'only', 'specifications', 'basicauthentication', 'item', 'trace', 'evolution', 'last', 'note', 'filtering', 'script', 'rapidly', 'explore', 'effectiveness', 'under', 'validation', 'quality', 'offering', 'analysts', 'timely', 'retention', 'below', 'capture', 'quick', 'frequently', 'external', 'evident', 'reflects', 'audiences', 'specify', 'output', 'factors', 'positive', 'nature', 'feature', 'designing', 'com', 'structured', 'constrains', 'assumes', 'gdpr', 'results', 'consideration', 'nuanced', 'revenue', 'holdout', 'facilitating', 'spark', 'governance', 'platform', 'construct', 'changing', 'univariate', 'application', 'implements', 'amplifies', 'categorize', 'methods', 'actionable', 'dormant', 'lawful', 'integration', 'transformative', 'marketing', 'understand', 'populated', 'tolerant', 'behaviours', 'users', 'purchase', 'restoration', 'responsibly', 'translating', 'datasets', 'imputing', 'reducing', 'reinforce', 'product', 'events', 'success', 'predict', 'ongoing', 'prioritizing', 'trails', 'metis', 'focusing', 'collaborative', 'guarantees', 'highly', 'trade', 'intricate', 'degree', 'stands', 'consolidates', 'considerations', 'verify', 'introduction', 'stacknexusdatanators', 'leverage', 'perception', 'promoting', 'continual', 'multiple', 'url', 'descriptive', 'customers', 'achieve', 'these', 'transaction', 'basket', 'are', 'measured', 'establishment', 'headers', 'weekly', 'across', 'shielding', 'origins', 'going', 'scalable', 'cross', 'api', 'leading', 'sources', 'work', 'map', 'configuration', 'activities', 'depends', 'derived', 'duplicate', 'solutions', 'plays', 'delves', 'recover', 'server', 'align', 'authentication', 'determined', 'variables', 'uncover', 'lifetime', 'meticulously', 'targeting', 'sustained', 'contribute', 'development', 'schema', 'extract', 'foundation', 'edge', 'rmf', 'addressing', 'diagram', 'scope', 'brokers', 'flare', 'professionals', 'entirety', 'action', 'clear', 'yield', 'volume', 'reviews', 'illuminates', 'effective', 'monitored', 'safeguard', 'finds', 'string', 'helps', 'engine', 'efficiency', 'regularly', 'clusters', 'works', 'derive', 'the', 'effectively', 'changes', 'empower', 'retrieval', 'with', 'advantage', 'provided', 'inconsistencies', 'unforeseen', 'loss', 'party', 'recent', 'likely', 'power', 'company', 'empowering', 'swiftly', 'yaml', 'base', 'processed', 'scale', 'interoperable', 'established', 'create', 'recency', 'channel', 'highlights', 'trends', 'assumptions', 'max', 'loyal', 'level', 'anomalies', 'category', 'exclusive', 'streaming', 'indicating', 'recommender', 'vulnerabilities', 'including', 'cpu', 'utilizing', 'needed', 'performing', 'meet', 'expected', 'further', 'tool', 'core', 'update', 'aimed', 'preprocessing', 'ultimately', 'accelerator', 'deriving', 'tools', 'market', 'chain', 'traceability', 'refine', 'expecting', 'paramount', 'ensure', 'description', 'disruptions', 'ownership', 'target', 'top', 'and', 'will', 'host', 'needs', 'customizable', 'compute', 'includes', 'nodes', 'where', 'regular', 'outlines', 'alterations', 'knowledge', 'sessions', 'control', 'significantly', 'evaluate', 'tasks', 'receiver', 'ensures', 'structures', 'guarantee', 'performance', 'address', 'port', 'nuances', 'name', 'logging', 'now', 'recognizes', 'assesses', 'resume', 'metadata', 'interconnections', 'solution', 'recommendation', 'they', 'typical', 'innovative', 'enhanced', 'consume', 'confidence', 'identification', 'hipaa', 'retail', 'characteristic', 'specific', 'utilize', 'pii', 'find', 'prevention', 'responsible', 'technical', 'enjoy', 'info', 'authorized', 'constraints', 'etc', 'add', 'architecture', 'ability', 'serve', 'industry', 'accessed', 'different', 'status', 'media', 'algorithms', 'package', 'transformation', 'memory', 'retaining', 'analytics', 'trino', 'functionalities', 'cycles', 'accuracy', 'reviewing', 'requirements', 'prevent', 'definition', 'pivotal', 'provide', 'clientele', 'majority', 'nlp', 'backup', 'engineers', 'detection', 'off', 'audience', 'meticulous', 'analyze', 'gender', 'see', 'assessments', 'responsibilities', 'job', 'communication', 'enriched', 'powering', 'track', 'completeness', 'conn', 'kafka', 'highlighting', 'shedding', 'long', 'improving', 'additionally', 'online', 'source', 'potentially', 'postgresql', 'syntax', 'prediction', 'harmoniously', 'showcases', 'limit', 'means', 'signifying', 'interpreting', 'reserves', 'ensuring', 'platforms', 'always', 'regulations', 'you', 'while', 'mechanisms', 'features', 'positives', 'pipeline', 'sales', 'encryption', 'instance', 'californian', 'generalize', 'implementing', 'contact', 'sensitive', 'partnerships', 'outcomes', 'transactional', 'selling', 'churn', 'resulting', 'secures', 'out', 'face', 'other', 'retrieves', 'regression', 'tolerance', 'can', 'mitigate', 'transform', 'segmented', 'enhancement', 'whose', 'normalization', 'model', 'targeted', 'seamlessly', 'capability', 'connects', 'structure', 'organizations', 'elevating', 'thorough', 'data', 'orders', 'informed', 'place', 'reliability', 'flow', 'require', 'associated', 'discriminatory', 'practices', 'adhering', 'encoding', 'seek', 'supply', 'built', 'egress', 'complexity', 'adherence', 'accurate', 'hidden', 'integrated', 'business', 'contributes', 'tcp', 'time', 'example', 'steward', 'requiring', 'more', 'engagement', 'powerful', 'connections', 'network', 'natural', 'details', 'schemas', 'views', 'govern', 'absence', 'rbac', 'reflecting', 'history', 'accountability', 'shown', 'return', 'experiences', 'line', 'similarities', 'hera', 'cleaning', 'scalability', 'serving', 'strategic', 'communicate', 'possible', 'filter', 'mitigating', 'minimizing', 'resource', 'investment', 'swift', 'tailor', 'what', 'stewardship', 'types', 'interact', 'incorporates', 'optimisation', 'exemplifies', 'datascientist', 'sophisticated', 'integrating', 'segmenting', 'frequent', 'growing', 'anticipate', 'explanantion', 'discretionary', 'undergo', 'visually', 'main', 'experts', 'masked', 'thereby', 'strike', 'bounce', 'connect', 'demographic', 'confidentiality', 'between', 'integrity', 'state', 'secure', 'clustering', 'recommend', 'segments', 'redshift', 'purpose', 'unauthorized', 'consistency', 'influencing', 'change', 'increased', 'disposal', 'node', 'against', 'frequencies', 'creates', 'fault', 'representation', 'relevant', 'uniqueness', 'mentioned', 'databases', 'that', 'steps', 'count', 'process', 'loops', 'interoperability', 'distill', 'specifically', 'frequency', 'easy', 'forecasting', 'implementation', 'each', 'guide', 'policies', 'offerings', 'identify', 'troubleshooting', 'enhancing', 'encouraging', 'dependencies', 'identified', 'probabilistic', 'identifiable', 'queries', 'following', 'resilience', 'balance', 'aims', 'applies', 'based', 'deeply', 'cores', 'responses', 'new', 'connection', 'aiding', 'value', 'habitual', 'cleansing', 'customer', 'cases', 'addon', 'handle', 'consolidating', 'predictions', 'who', 'definitions', 'discoverability', 'optimize', 'for', 'stacks', 'measures', 'complete', 'demands', 'understanding', 'overseeing', 'upholding', 'undergoes', 'availability', 'breaches', 'clickstream', 'personalized', 'parents', 'driven', 'personnel', 'advanced', 'ingress', 'functions', 'minimum', 'continuous', 'criteria', 'avoid', 'monetary', 'odin', 'patterns', 'prompt', 'preferences', 'feedback', 'keep', 'unified', 'missing', 'but', 'reduced', 'access', 'industries', 'utilized', 'appointment', 'offer', 'showcasing', 'redundancy', 'lens', 'engineering', 'least', 'aiming', 'timeliness', 'behavior', 'range', 'reliable', 'deep', 'date', 'cutting', 'recorded', 'here', 'shared', 'diverse', 'risks', 'open', 'rapid', 'workflow', 'any', 'rows', 'score', 'deletions', 'california', 'risk', 'unveils', 'testing', 'operating', 'performed', 'python', 'encompassing', 'acknowledges', 'faceted', 'serves', 'age', 'roc', 'supports', 'transit', 'limitations', 'regulatory', 'tactics', 'key', 'handling', 'detailed', 'attrition', 'used', 'yamls', 'configurations', 'eliminated', 'like', 'major', 'use', 'broker', 'should', 'thus', 'third', 'influences', 'statistical', 'beacon', 'forecast', 'those', 'blob', 'related', 'instances', 'making', 'uses', 'brief', 'predictive', 'enabling', 'capacity', 'explanation', 'eda', 'checks', 'implement', 'procedures', 'minervac', 'fields', 'transforming', 'conversion', 'trust', 'fortify', 'come', 'operational', 'security', 'tmdc', 'failures', 'downtime', 'aligns', 'efficacy', 'per', 'table', 'contribution', 'accurately', 'enables', 'analysing', 'accommodate', 'ide', 'entities', 'recall', 'actual', 'spanning', 'not', 'lasting', 'trees', 'identifying', 'cluster', 'principles', 'remains', 'dimensions', 'elevates', 'illustrates', 'trinosql', 'records', 'query', 'leverages', 'https', 'avaxtilable', 'demonstrates', 'demographics', 'assessment', 'designed', 'metrics', 'area', 'clearly', 'foster', 'some', 'cyber', 'standardization', 'devise', 'restrict', 'components', 'services', 'balaji', 'various', 'select', 'strikes', 'relational', 'design', 'campaign', 'elaborated', 'precision', 'recovery', 'backups', 'limitation', 'their', 'characteristics', 'employing', 'dashboards', 'behaviors', 'underscoring', 'employs', 'profiles', 'firstly', 'include', 'campaigns', 'satisfaction', 'light', 'navigate', 'migrated', 'fulfilment', 'maximum', 'organization', 'leveraging', 'analysis', 'correctness', 'false', 'businesses', 'total', 'underline', 'currently', 'spending', 'subject', 'safeguarding', 'environment', 'most', 'apache', 'respond', 'technologies', 'allow', 'allows', 'maintain', 'evaluates', 'periods', 'reporting', 'stewards', 'decisions', 'outages', 'roles', 'responsiveness', 'using', 'reflect', 'vast', 'offers', 'contributions', 'reflected', 'utilization', 'intricacies', 'make', 'maximizes', 'granular', 'documentation', 'overview', 'repository', 'active', 'privacy', 'activity', 'metric', 'domains', 'such', 'anomaly', 'rfm', 'encompasses', 'has', 'about', 'networks', 'there', 'aligning', 'processing', 'touchpoints', 'servers', 'individual', 'conducted', 'efforts', 'historical', 'represent', 'allowing', 'assumption', 'demand', 'encapsulates', 'exhaustive', 'strategies', 'infrastructure', 'from', 'framework', 'loyalty', 'facts', 'suggest', 'brand', 'disaster', 'legal', 'equip', 'immediate', 'all', 'full', 'corresponding', 'best', 'aravind', 'several', 'sample', 'group', 'updated', 'portion', 'accessing', 'tailored', 'rate', 'maggot', 'profile', 'scenarios', 'architectural', 'streams', 'potential', 'represents', 'this', 'ext', 'graph', 'facilitates', 'discrepancies', 'service', 'curve', 'certain', 'stipulated', 'empowered', 'engaged', 'machine', 'software', 'scenario', 'ethical', 'scientists', 'reach', 'learning', 'size', 'analytical', 'appoint', 'jobs', 'joby', 'current', 'collectively', 'utility', 'inclusion', 'unusual', 'context', 'systems', 'rest', 'rigorously', 'neural', 'accidental', 'spec', 'found', 'fraud', 'audits', 'benefits', 'which', 'comprehend', 'made', 'check', 'support', 'allocation', 'critical', 'illustrating', 'modifications', 'analyzing', 'rates', 'enough', 'segmentation', 'into', 'fosters', 'protection', 'depend', 'input', 'predictability', 'threats', 'pertinent', 'tailoring', 'along', 'useful', 'behaviour', 'elements', 'segment', 'delineation', 'formats', 'utilizes', 'enhance', 'migration', 'growth', 'dac', 'values', 'play', 'completness', 'immediately', 'aggregating', 'section', 'unnecessary', 'compliance', 'blend', 'boosting', 'fold', 'raw', 'above', 'order', 'aws', 'plan', 'updates', 'informing', 'disparate', 'payment', 'products', 'standard', 'items', 'facilitate', 'columns', 'aspect', 'resources', 'coherent', 'improve', 'levels', 'also', 'science', 'smooth', 'large', 'points', 'outliers', 'provides', 'financial', 'objectives', 'continuously', 'realm', 'techniques', 'daily', 'interactions', 'google', 'harnesses', 'endpoint', 'strong', 'depth', 'holistic', 'forming', 'database', 'specificity', 'generated', 'through', 'modeling', 'captures', 'robust', 'sections', 'view', 'does', 'crucial', 'during', 'term', 'define', 'happen', 'null', 'high', 'together', 'impact', 'applications', 'sheet', 'evolving', 'store', 'supporting', 'hourly', 'controls', 'experience', 'document', 'sensitivity', 'comprehensive', 'site', 'detailing', 'relationship', 'dataset', 'adaptation', 'apply', 'lineage', 'content', 'eliciting', 'producers', 'occur', 'overall', 'providing', 'applied', 'assess', 'basic', 'protocols', 'rectify', 'driving', 'involved', 'vital', 'how', 'lifecycles', 'method', 'seamless', 'within', 'engagements', 'may', 'number', 'social', 'trend', 'series', 'subsets', 'instils', 'improved', 'user', 'underscored', 'essence', 'management', 'uniformity', 'involve', 'amount', 'opportunities', 'groups', 'inherently', 'hub', 'deployment', 'perspective', 'categorical', 'two', 'ten', 'version', 'github', 'increase', 'stringent', 'web', 'due', 'processes', 'tracking', 'proactive', 'crm', 'auth', 'system', 'ram', 'guaranteeing', 'span', 'correctly', 'destination', 'ntify', 'transactions', 'granted', 'contexts', 'brands', 'multivariate', 'inventory', 'usecases', 'format', 'generation', 'diagrams', 'involves', 'larger', 'storage', 'auc', 'link', 'periodic', 'masking', 'upselling', 'implemented', 'uniqness', 'purchasing', 'programs', 'visual', 'dynamic', 'monitoring', 'monitor', 'personally', 'launches', 'both', 'optimization', 'cheerful', 'information', 'necessary', 'entire', 'decision', 'entity', 'real', 'enable']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_topics(html_file_path):\n",
    "    with open(html_file_path, 'r') as f:\n",
    "        html_data = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_data, 'html.parser')\n",
    "    topics = set()\n",
    "\n",
    "    for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):\n",
    "        text = ' '.join(tag.get_text().lower().split())\n",
    "        words = re.findall(r'\\w+', text)\n",
    "        for word in words:\n",
    "            if word.isalpha() and len(word) > 2:\n",
    "                topics.add(word)\n",
    "\n",
    "    return list(topics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html_file_path = \"doc.html\"\n",
    "    topics = extract_topics(html_file_path)\n",
    "    print(\"Topics:\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f56bb256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content of 'Overview of the Data Product': \n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_topic_info(file_path, topic):\n",
    "    doc = Document(file_path)\n",
    "    topic_paragraphs = [p for p in doc.paragraphs if p.text.strip() == topic]\n",
    "    if topic_paragraphs:\n",
    "        topic_info = '\\n'.join([p.text for p in topic_paragraphs[1:]])\n",
    "        return topic_info\n",
    "    return None\n",
    "\n",
    "file_path = 'doc.docx'\n",
    "topic = \"Overview of the Data Product\"\n",
    "topic_info = extract_topic_info(file_path, topic)\n",
    "print(f\"Text content of '{topic}': {topic_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3e0a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_topic_info_from_html(html_content, topic):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    topic_element = soup.find('strong', text=topic)\n",
    "    if topic_element:\n",
    "        topic_info = topic_element.find_next_sibling().get_text().strip()\n",
    "        return topic_info\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15e2b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'High-Level Description of the Data Product' not found in the document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/2500972111.py:7: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  topic_element = soup.find('strong', text=topic)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_topic_info_from_html(file_path, topic):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        topic_element = soup.find('strong', text=topic)\n",
    "        if topic_element:\n",
    "            next_sibling = topic_element.find_next_sibling()\n",
    "            if next_sibling:\n",
    "                topic_info = next_sibling.get_text().strip()\n",
    "                return topic_info\n",
    "        return None\n",
    "\n",
    "file_path = 'doc.html'\n",
    "topic = \"High-Level Description of the Data Product\"\n",
    "topic_info = extract_topic_info_from_html(file_path, topic)\n",
    "\n",
    "if topic_info:\n",
    "    print(f\"Text content of '{topic}':\\n{topic_info}\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66aac5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content of 'Purpose and Objectives':\n",
      "Purpose and Objectives \n",
      "The primary purpose of the Customer 360 Data Product is to equip organizations with a powerful tool that facilitates a nuanced understanding of their customer base. By aggregating and analysing diverse datasets, the product aims to achieve the following objectives: \n",
      "  \n",
      "Enhance Customer Engagement: Enable personalized and targeted customer interactions by leveraging insights derived from a 360-degree view of customer profiles. \n",
      "  \n",
      "Optimize Marketing Strategies: Facilitate data-driven marketing campaigns, segmentation, and targeting, resulting in improved campaign effectiveness and customer conversion rates. \n",
      "  \n",
      "Drive Operational Efficiency: Support decision-making processes by providing actionable insights to various business functions, ultimately improving operational efficiency and resource allocation. \n",
      "  \n",
      "Increase Customer Retention and Loyalty: Ide\tntify at-risk customers, implement proactive retention strategies, and enhance overall customer satisfaction, fostering loyalty and repeat business. \n",
      "  \n",
      "Target Audience \n",
      "The Customer 360 Data Product is tailored to meet the needs of diverse professionals involved in data analysis, engineering, and science. The primary target audience includes: \n",
      "  \n",
      "Business Analysts: Individuals responsible for interpreting and translating business requirements into data-driven insights. Business analysts will leverage the product to derive actionable recommendations for enhancing customer experiences and driving business outcomes. \n",
      "  \n",
      "Data Engineers: Professionals tasked with designing, implementing, and maintaining the data infrastructure. Data engineers will play a crucial role in ensuring seamless data integration, reliability, and real-time updates within the Customer 360 Data Product. \n",
      "  \n",
      "Data Scientists: Experts in statistical analysis, machine learning, and predictive modeling who will utilize the product to uncover hidden patterns, forecast customer behaviors, and contribute to the development of advanced analytics solutions. \n",
      "  \n",
      "Document Version and Change History \n",
      "Document Version: 1.0 \n",
      "Last Updated: 12-12-2023 \n",
      "  \n",
      "Product Description \n",
      "High-Level Description of the Data Product \n",
      "The Customer 360 Data Product is a cutting-edge solution designed to provide organizations with a unified and dynamic view of their customers. At its core, the product consolidates data from diverse sources, including CRM systems, sales transactions, customer support interactions, and online engagements, to create a comprehensive 360-degree view of each customer. This high-level description encapsulates the essence of a transformative tool that enables businesses to gain profound insights into customer behavior, preferences, and interactions. \n",
      " \n",
      "Key Features and Capabilities \n",
      "1. Data Integration: \n",
      "Seamless integration of data from multiple sources, ensuring a unified and enriched customer profile. \n",
      " \n",
      "2. 360-Degree Customer Profile: \n",
      "Comprehensive customer profiles encompassing demographic information, purchase history, communication preferences, and more. \n",
      " \n",
      "3. Real-time Updates: \n",
      "Continuous and real-time updates to keep customer profiles current, facilitating prompt responses to changing customer behaviors. \n",
      " \n",
      "4. Advanced Analytics and Reporting: \n",
      "Powerful analytics tools and customizable reporting dashboards for deriving actionable insights from customer data. \n",
      " \n",
      "5. Segmentation and Targeting: \n",
      "Tools for segmenting customers based on various criteria, enabling personalized marketing and communication strategies. \n",
      " \n",
      "6. Predictive Analytics: \n",
      "Utilization of predictive modeling and machine learning algorithms to forecast customer behaviors and recommend personalized offerings. \n",
      " \n",
      "7. Security and Compliance: \n",
      "Stringent security measures and data governance protocols to ensure the accuracy, reliability, and privacy of customer information. \n",
      " \n",
      "Business Context and Application \n",
      "The Customer 360 Data Product finds application across diverse business contexts, including: \n",
      " \n",
      "Marketing Optimization: Tailoring marketing campaigns based on customer insights to enhance targeting and campaign effectiveness. \n",
      " \n",
      "Customer Retention: Implementing proactive retention strategies by identifying at-risk customers and addressing their needs. \n",
      " \n",
      "Operational Efficiency: Supporting decision-making processes and resource allocation through data-driven insights. \n",
      " \n",
      "Personalized Customer Experiences: Enabling personalized interactions and experiences by leveraging a deep understanding of individual customer preferences. \n",
      " \n",
      "Expected Outcomes and Benefits \n",
      "Implementing the Customer 360 Data Product is expected to yield the following outcomes and benefits: \n",
      " \n",
      "Improved Customer Satisfaction: Enhanced customer experiences through personalized interactions and targeted communication. \n",
      " \n",
      "Increased Revenue: Identification of cross-selling and upselling opportunities, leading to increased revenue generation. \n",
      " \n",
      "Reduced Churn: Proactive identification and addressing of at-risk customers, resulting in reduced churn rates. \n",
      " \n",
      "Data-Driven Decision-Making: Empowering organizations with data-driven insights for strategic decision-making. \n",
      " \n",
      "Enhanced Marketing ROI: Optimizing marketing strategies and campaigns for improved return on investment. \n",
      " \n",
      "This section provides a detailed overview of the Customer 360 Data Product, highlighting its key features, capabilities, business applications, and the expected outcomes and benefits for organizations leveraging this transformative solution. \n",
      "  \n",
      "  \n",
      "Technical Specifications \n",
      " \n",
      "Customer 360 Lens\n",
      "Recency Frequency and Monitoring (RMF) Analysis:\n",
      "The integration of RFM (Recency, Frequency, Monetary Value) analysis within Customer 360 (C360) lenses has empowered businesses to deeply understand customer behaviour and tailor marketing strategies effectively. By analyzing recent interactions (Recency), frequency of engagement (Frequency), and monetary contributions (Monetary Value), C360 segments customers into groups. This segmentation helps identify high-value, loyal customers for personalized engagement strategies. It also highlights dormant segments requiring re-engagement tactics to prevent potential churn. Utilizing RFM within C360 lenses elevates targeted marketing, enhances customer experiences, and fosters long-term loyalty, driving revenue growth and informed decision-making.\n",
      "Recency: Recency in RFM analysis within C360 lenses illuminates recent customer engagements, indicating their current activity levels and potential ongoing interactions. This metric enables businesses to identify active customers more likely to respond to immediate marketing strategies or personalized offers. Leveraging recency insights allows for targeted engagement, potentially boosting immediate sales or eliciting prompt customer responses, thus elevating overall customer satisfaction and engagement levels.\n",
      "Frequency: Frequency analysis in C360 lenses delves into customer engagement consistency, signifying customer loyalty and habitual interactions. This aspect identifies highly engaged customers, forming the foundation for strategies aimed at retaining loyalty and encouraging repeat interactions. Segmentation and understanding of high-frequency customers empower businesses to design targeted marketing efforts and loyalty programs tailored to maintain and potentially increase these customers' lifetime value, ensuring sustained revenue streams.\n",
      "Monitoring: within C360 lenses' Monetary Value assessment, involves continual tracking of customer transactions and their impact on overall revenue. This process identifies high-value customers whose spending significantly influences business success. Through ongoing monitoring, tailored strategies like exclusive offers or personalized services are applied to retain and increase spending from these customers. Prioritizing enhanced experiences for such clientele maximizes their contribution to the company's financial success.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Sources and Formats \n",
      " \n",
      "Datasets  \n",
      " Customer  \n",
      "Product  \n",
      "Orders  \n",
      "Transactions  \n",
      "Order line item  \n",
      "Web Analytics  \n",
      "Brands  \n",
      "Entity Relationship Diagram \n",
      "\n",
      "\n",
      " Data Sources   \n",
      "AWS Redshift \n",
      "PostgreSQL \n",
      "Web Analytics (Google)  \n",
      " \n",
      " \n",
      "Data Volume and Update Frequency (Balaji)\n",
      "The volume of data processed by the Customer 360 Data Product depends on the scale and scope of the organization. Factors influencing data volume include the customer base size, transaction frequency, and engagement levels. Update frequencies vary based on the source but are optimized for real-time updates to ensure the most current customer profiles. \n",
      " \n",
      "System Requirements and Dependencies \n",
      " \n",
      "Compute Cluster: Apache Spark \n",
      "Number of Nodes: 3 nodes \n",
      "Node Type: High-memory instances (e.g., r5.4xlarge) with at least 16 CPU cores and 64 GB RAM per node. \n",
      "Total CPU Cores: 48 cores \n",
      "Total RAM: 192 GB \n",
      "Apache Kafka Cluster \n",
      "Number of Brokers: 3 brokers (for redundancy and fault tolerance) \n",
      "Broker Instance Type: Instance type with at least 8 CPU cores and 32 GB RAM per broker. \n",
      "Total CPU Cores: 24 cores \n",
      "Total RAM: 96 GB \n",
      "PostgreSQL Database \n",
      "Instance Type: High-memory instance (e.g., r5.xlarge) with at least 4 CPU cores and 16 GB RAM. \n",
      "Storage: Enough storage capacity to handle the \"orders_enriched_data\" dataset. \n",
      " \n",
      " \n",
      "Data Flow and Architecture Diagram \n",
      " \n",
      "Security and Compliance Considerations \n",
      "Security Measures \n",
      "Encryption: Data encryption in transit and at rest to ensure the confidentiality and integrity of customer information. \n",
      "Access Control: Role-based access control mechanisms to restrict data access based on user roles. \n",
      "Audit Trails: Logging and monitoring features to track user activities and changes to customer profiles. \n",
      "Compliance Considerations \n",
      "Data Privacy Regulations: Adherence to data protection regulations such as GDPR, HIPAA, or industry-specific compliance standards. \n",
      "Data Retention Policies: Implementation of policies for the secure storage and responsible disposal of customer data. \n",
      " \n",
      " \n",
      "Built Using \n",
      " \n",
      "Python: For data processing, transformation, and integration tasks. \n",
      "Apache Kafka: To enable real-time streaming of customer order events. \n",
      "Apache Spark: For handling large-scale data processing efficiently. \n",
      "PostgreSQL: As the primary database to store the enriched order data.  \n",
      " \n",
      "Data Models and Algorithms \n",
      "Description of Data Models Used \n",
      "The Customer 360 Data Product employs a sophisticated blend of data models to construct a comprehensive and unified representation of customer profiles: \n",
      " \n",
      "1. Entity-Relationship Model: \n",
      "Illustrates the relationships and connections between entities, such as customers, transactions, and interactions. \n",
      "2. Graph Model: \n",
      "Captures the intricate connections and dependencies between various elements, providing a visual map of customer relationships and network structures. \n",
      "3. Predictive Models: \n",
      "Harnesses machine learning algorithms, including regression analysis, decision trees, and neural networks, to anticipate customer behaviours and uncover patterns. \n",
      "4. Segmentation Models: \n",
      "Utilizes algorithms like k-means clustering to categorize customers based on shared characteristics, enabling targeted marketing and personalized engagement. \n",
      " \n",
      " \n",
      "Algorithms and Analytical Methods \n",
      "1. Predictive Analytics: \n",
      "Applies advanced algorithms to predict customer behaviours, preferences, and trends, enabling proactive decision-making. \n",
      "Predictive Sales \n",
      "Predictive Churn \n",
      "2. Clustering Algorithms: \n",
      "Utilizes k-means clustering and other algorithms to identify patterns and group customers based on similarities, facilitating tailored strategies. \n",
      " \n",
      "3. Recommendation Algorithms: \n",
      "Implements collaborative filtering and content-based recommendation algorithms to suggest personalized products and services. \n",
      "Market Basket Analysis Based product recommendation \n",
      "Marketing Channel Optimisation and recommendation \n",
      "4. Descriptive Analytics: \n",
      "Leverages statistical methods and analytics tools to derive meaningful insights from historical customer data, aiding in comprehensive understanding. \n",
      " \n",
      " \n",
      "Assumptions and Limitations \n",
      "Assumptions \n",
      "Quality of Input Data: Assumes that the accuracy and reliability of models depend on the quality and completeness of the input data. \n",
      " \n",
      "Predictability: Assumes a degree of predictability in customer behavior based on historical patterns. \n",
      " \n",
      "Limitations \n",
      "Probabilistic Nature: Acknowledges that predictive models are inherently probabilistic and may not always accurately forecast customer behaviors. \n",
      " \n",
      "Data Constraints: Recognizes that data models may not capture all nuances of customer interactions or external factors due to data limitations. \n",
      " \n",
      "Model Validation and Performance Metrics \n",
      "Model Validation \n",
      "Cross-Validation: Utilizes techniques like k-fold cross-validation to assess model performance across different subsets of the dataset. \n",
      " \n",
      "Holdout Validation: Reserves a portion of the dataset for validation to evaluate the model's ability to generalize to new data. \n",
      " \n",
      "Performance Metrics \n",
      "Accuracy: Measures the overall correctness of predictions. \n",
      " \n",
      "Precision and Recall: Evaluates the model's ability to correctly identify positive instances and avoid false positives. \n",
      " \n",
      "F1 Score: Strikes a balance between precision and recall, providing a holistic performance metric. \n",
      " \n",
      "Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC): Assesses the trade-off between sensitivity and specificity, offering insights into the model's discriminatory power. \n",
      " \n",
      "This section unveils the intricacies of the data models and algorithms powering the Customer 360 Data Product, shedding light on the methods employed to create a unified customer perspective, predict behaviours, and segment \n",
      " \n",
      "User Guide \n",
      " \n",
      "Accessing the Data Product \n",
      " \n",
      "The data product can be accessed from the Data Product hub on the corresponding DataOS instance.  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Advanced Features (for Data Scientists) \n",
      " \n",
      "Data Scientists can enjoy majority of their EDA through METIS and Odin which has all the major Univariate and Multivariate analysis in place.  \n",
      " \n",
      "Troubleshooting and Support \n",
      " \n",
      "If they need any support on the data product, they can reach out to Balaji at balaji.ext@tmdc.io, Aravind at aravind@tmdc.io \n",
      " \n",
      " \n",
      "Data Quality and Governance \n",
      " \n",
      "Data Quality Measures and Standards \n",
      "Data quality in the Customer 360 Data Product will be rigorously measured through a multi-faceted approach encompassing various dimensions. Firstly, accuracy checks will be conducted to ensure that the information within customer profiles aligns with the actual interactions and transactions recorded across different sources. Completeness assessments will verify the presence of essential data points, promoting a holistic customer view. Timeliness will be monitored to ensure that customer profiles reflect real-time updates, allowing for timely and relevant insights. Consistency checks will be employed to identify any discrepancies or anomalies in data patterns, while uniqueness measures will ensure the absence of duplicate records. Additionally, data quality audits will be conducted regularly to address any discrepancies, and feedback loops will be established to continuously enhance data accuracy and reliability. Through these comprehensive measures, the Customer 360 Data Product aims to maintain a high standard of data quality, fostering trust in the information provided for strategic decision-making and operational efficiency. \n",
      " \n",
      "Data Cleansing and Preprocessing Steps \n",
      " \n",
      "Data cleaning and preprocessing in the Customer 360 Data Product involve a meticulous process to ensure the quality and integrity of the information. Initially, raw data from diverse sources undergoes cleansing procedures, addressing missing values, outliers, and inconsistencies. Standardization and normalization techniques are applied to ensure uniformity, while duplicate records are identified and eliminated. Data preprocessing involves transforming and aggregating information to create a cohesive dataset. Advanced algorithms are employed to handle complex tasks such as imputing missing values and encoding categorical variables. Through this thorough cleaning and preprocessing pipeline, the Customer 360 Data Product guarantees a reliable foundation for robust analytics and actionable insights. \n",
      " \n",
      "\n",
      "Data Governance Policies and Data Stewardship\n",
      "There are two types of Data Governance policies:\n",
      "Role-Based Access Control (RBAC):\n",
      "Users see only corresponding rows and datasets based on their access role.\n",
      "Discretionary Access Control (DAC):\n",
      "Currently, DAC (full access) is granted to:\n",
      "For RBAC:\n",
      "Certain Personally Identifiable Information (PII) is masked for each category.\n",
      "Data Steward Responsibilities:\n",
      "Clearly define roles and responsibilities for data ownership.\n",
      "Appoint data stewards responsible for overseeing data quality and integrity.\n",
      "\n",
      " \n",
      "Data Lineage and History Tracking \n",
      " \n",
      "In DataOS, the data lineage of the Custome360 product can be accessed through the Hera application within DataOS. The change history and history tracking is possible through METIS.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Customer 360 Lens \n",
      " \n",
      "The customer 360 Lens is built by connecting the entities mentioned above as shown in the diagram below,  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "The definition of the lens can be found in the following URL \n",
      "https://github.com/StacknexusDatanators/retail-accelerator/blob/main/dataos_yamls/lens/c360_solution_accelerator.yaml \n",
      " \n",
      " \n",
      "Integration and Deployment \n",
      " \n",
      "Integration with Other Systems (for Data Engineers) \n",
      " \n",
      "The data can be integrated with any other data system through various methods,  \n",
      " \n",
      "Spark based Flare jobs where the raw datasets can be integrated or migrated to their destination \n",
      "Flare is a Data Migration Engine Built on top of Apache Spark. A typical Flare YAML has the following Structure,  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "You can find more information about flare in the following link \n",
      "(https://dataos.info/resources/stacks/flare/#syntax-of-flare-yaml-configuration) \n",
      " \n",
      "Here is an example flare job for an enriched orders dataset that can be built from the Customer360 Data Product  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "You can access the source YAMLs in the following github repository.  \n",
      " \n",
      "https://github.com/StacknexusDatanators/retail-accelerator \n",
      " \n",
      " \n",
      "TrinoSQL based query connection to all the datasets and lenses avaxtilable \n",
      " (NEED TO COMPLETE)\n",
      " \n",
      "Scalability and Performance Considerations \n",
      "Scalability and performance considerations in the Customer 360 Data Product involve optimizing infrastructure to accommodate growing datasets and user demands. Implementing scalable architecture ensures responsiveness, allowing the platform to efficiently handle increased data volume, user interactions, and evolving analytics needs, fostering a seamless and high-performance experience. \n",
      " \n",
      "Backup and Disaster Recovery Procedures \n",
      " \n",
      "Backup and disaster recovery strategies for the Customer 360 Data Product are pivotal for safeguarding against unforeseen events. Regular backups of critical data are conducted to secure against accidental deletions, system failures, or cyber threats. An effective disaster recovery plan outlines procedures for swift data restoration, minimizing downtime. Off-site data storage and redundancy measures are implemented to enhance resilience. Periodic testing of recovery procedures ensures their efficacy, guaranteeing that the Customer 360 Data Product can swiftly recover from disruptions, maintain data integrity, and resume operations, thereby safeguarding against potential data loss or system outages. \n",
      " \n",
      " \n",
      "Use Cases and Scenarios \n",
      " \n",
      "Typical Use Cases for Business Analysis \n",
      " \n",
      "1. Personalized Marketing Campaigns: \n",
      "Analyze customer preferences and behavior to tailor marketing campaigns for increased engagement and conversion rates. \n",
      " \n",
      "2. Customer Segmentation: \n",
      "Utilize segmentation models to categorize customers based on demographics and behaviors, optimizing targeted marketing strategies. \n",
      " \n",
      "3. Churn Prediction and Retention Strategies: \n",
      "Implement predictive analytics to identify at-risk customers and devise proactive retention strategies, reducing churn rates. \n",
      " \n",
      "4. Sales Performance Analysis: \n",
      "Evaluate sales data to identify top-performing products, optimize pricing strategies, and enhance overall sales performance. \n",
      " \n",
      "5. Customer Satisfaction Analysis: \n",
      "Analyze customer feedback and support interactions to assess satisfaction levels and improve customer service strategies. \n",
      " \n",
      "Advanced Analytical Scenarios for Data Scientists \n",
      " \n",
      "1. Predictive Modeling for Revenue Forecasting: \n",
      "Develop and deploy predictive models to forecast revenue based on historical data and market trends. \n",
      " \n",
      "2. Anomaly Detection for Fraud Prevention: \n",
      "Implement advanced anomaly detection algorithms to identify unusual patterns indicative of potential fraud in transactions. \n",
      " \n",
      "3. Natural Language Processing (NLP) for Customer Sentiment Analysis: \n",
      "Apply NLP techniques to analyze customer reviews and social media data for sentiment analysis, informing brand perception strategies. \n",
      " \n",
      "4. Machine Learning Recommender Systems: \n",
      "Develop personalized recommender systems using machine learning algorithms for accurate product or content recommendations. \n",
      " \n",
      "5. Time Series Analysis for Demand Forecasting: \n",
      "Utilize time series analysis to forecast customer demand, optimizing inventory management and supply chain operations. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What is a Data Product?\n",
      "A data product is a package of data, metadata, characteristics of the data, means of use, Governance, Quality. \n",
      "Data Products Spec sheet:\n",
      "A data product spec sheet will also include the usecases, scenarios and other information..\n",
      "Data product spec sheet encapsulates the entirety of information that users seek and consume about the product, including exhaustive details on its sources, technologies employed, use cases, and generated insights. This encompasses all relevant data-driven insights, aims, and functionalities, serving as a holistic repository of knowledge vital for understanding the product's utility from a business perspective. Data product spec sheet details use cases such as: spanning software applications, platforms, algorithms, or systems, serve to transform raw data into actionable insights, optimizing processes, enhancing user experiences, and fostering informed decision-making across diverse industries and domains.\n",
      "Data Product platform:\n",
      "It supports ingress data interoperability and egress data interoperability. \n",
      "Interoperability:\n",
      "The Customer 360 Data Product demonstrates strong interoperability by seamlessly integrating and consolidating data from disparate sources, including AWS Redshift, PostgreSQL, and Google Web Analytics. This interoperability ensures that data from various formats and platforms can be harmoniously processed and utilized within the system. Additionally, the use of technologies such as Apache Spark, Apache Kafka, and PostgreSQL databases in the system requirements guarantees that these components interact efficiently, allowing for smooth data flow and processing across the entire ecosystem. The architectural diagrams visually represent the interconnections, showcasing the system's interoperability by illustrating how different components communicate and work together to create a unified view of customer data.\n",
      "Reliability:\n",
      "Reliability in the Customer 360 Data Product is evident through its emphasis on real-time updates and robust system configurations. The continuous and real-time updates to customer profiles ensure the accuracy and responsiveness of the system, reflecting its reliability in reflecting current customer behaviors and preferences. Specifications detailing the Compute Cluster, Apache Kafka Cluster, and PostgreSQL Database, with specific node configurations and redundancy measures, reinforce the system's reliability by ensuring high availability and fault tolerance. The architecture's fault-tolerant design, along with redundancy in clusters, contributes to the system's resilience against failures, further enhancing reliability.\n",
      "Governance:\n",
      "Governance within the Customer 360 Data Product encompasses comprehensive security measures, compliance considerations, and structured data stewardship. The product incorporates robust security measures, including encryption for data in transit and at rest, role-based access controls (RBAC), and stringent audit trails. These measures ensure data integrity, confidentiality, and regulatory compliance (such as GDPR, HIPAA) while mitigating risks associated with unauthorized access or data breaches. The delineation of governance policies like Data Lineage and History Tracking through applications like Hera and METIS showcases the system's adherence to governance principles, facilitating traceability and accountability across data lifecycles. The clear definition of data ownership roles, appointment of data stewards, and data lineage mechanisms underline the product's commitment to governance best practices.\n",
      "Discoverability:\n",
      "Discoverability within the Customer 360 Data Product is underscored by its detailed technical specifications, comprehensive documentation, and traceability mechanisms. The product's exhaustive documentation encompasses detailed information about features, data sources, system requirements, and dependencies, facilitating easy access and understanding for users involved in data analysis, engineering, and science. The inclusion of tools like Hera and METIS for data lineage and history tracking amplifies the system's discoverability, enabling users to navigate through the data ecosystem, trace data origins, and comprehend the historical evolution of customer profiles. Additionally, the RBAC and DAC policies ensure that users can access and explore data within their prescribed access policies, further enhancing discoverability while safeguarding sensitive information.\n",
      "Definition for C360\n",
      "The Customer 360 is an innovative data solution designed to provide organizations with a unified, in-depth understanding of their customer base. By consolidating data from various sources like CRM systems, sales transactions, and online interactions, the C360 creates comprehensive customer profiles, detailing demographics, purchase history, and preferences. This holistic view enables businesses to enhance customer experiences, refine marketing strategies, and make informed decisions. Tailored for data professionals, it enables seamless integration and advanced analytics, fostering personalized interactions, operational efficiency, and lasting customer loyalty. The C360 stands as a powerful tool leveraging data for precise insights and strategic advantage.\n",
      "DATA SCHEMAS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Security: C360 employs a security framework encompassing encryption, access control, and audit trails. Through advanced encryption techniques, the product secures data both during transit and at rest, shielding sensitive customer information from unauthorized access or breaches. Access control mechanisms, such as Role-Based Access Control (RBAC) and Discretionary Access Control (DAC), limit data access based on user roles. This ensures that only authorized personnel, like Data Stewards or Data Producers, can access specific information while employing data masking techniques for sensitive Personally Identifiable Information (PII). Detailed audit trails track and monitor user activities and changes made to customer profiles, fostering accountability and enabling rapid identification of any potential security threats or unauthorized alterations. These robust security measures collectively fortify the platform against vulnerabilities and ensure the protection of customer data from various threats.\n",
      "Compliance: C360 aligns with pertinent compliance standards and regulations, underscoring its commitment to data privacy, protection, and responsible data management. Adhering to stringent data privacy regulations such as GDPR and HIPAA, the product implements stringent measures to safeguard customer data, ensuring lawful and ethical handling practices. Additionally, the establishment of data retention policies within C360 ensures secure data storage and responsible disposal, in compliance with stipulated retention periods and privacy requirements. These compliance efforts not only foster customer trust by upholding data integrity and privacy but also mitigate risks associated with legal and regulatory implications.\n",
      "By integrating security measures and aligning with stringent compliance standards, the Customer 360 data product not only guarantees data confidentiality and integrity but also ensures ethical data handling practices. This comprehensive approach to security and compliance within C360 instils confidence in users and organizations, fostering trust in the platform's capability to handle sensitive customer information responsibly and securely across its data ecosystem.\n",
      "\n",
      "\n",
      "Example how a lens query works:\n",
      "The query exemplifies a Customer 360 lens in action, showcasing its ability to extract specific insights. By focusing on Californian customers, it retrieves details such as the total customer count and segmented profiles like \"High Spending Customers\" or \"Expecting Parents.\" This demonstrates the lens's capability to distill vast datasets into actionable customer attributes, facilitating targeted decision-making and personalized engagement strategies.\n",
      "Sample Query:\n",
      "Sample Output:\n",
      "\n",
      "Sample of a TrinoSQL based query connection:\n",
      "\n",
      "The Python script connects to a TrinoSQL server and queries ten records from the 'clickstream' table in the 'retail_acclerator' database. This connection uses HTTPS authentication with specific headers for access, allowing easy data retrieval from TrinoSQL servers using Python.\n",
      "\n",
      "Sample of TrinoSQL  connection in python and query: \n",
      "\n",
      "Beacon:\n",
      "The Beacon API endpoint, serves a crucial role in the realm of retail analytics. Specifically designed for churn prediction, this API retrieves data from a PostgreSQL database, a robust open-source relational database system. Operating within the DataOS platform, the API is integral to forecasting customer churn in retail scenarios. By leveraging the capabilities of PostgreSQL and the DataOS environment, it plays a key role in providing valuable insights for businesses aiming to anticipate and address customer attrition effectively.\n",
      "\n",
      "Data Quality Metrics: \n",
      "Ensuring the reliability and accuracy of data is paramount for the effectiveness of the Customer 360 Data Product. To maintain high data quality, regular checks and assessments are conducted on each dataset. Key quality metrics include:\n",
      "Null Values: Regular checks are performed to identify and address any null values within the datasets. Null values can impact analysis and decision-making, so their presence is meticulously monitored.\n",
      "Completeness: Assessments are made to ensure that each dataset is complete, with all necessary fields populated. This metric helps guarantee that there are no missing components crucial for a comprehensive understanding of customer profiles.\n",
      "Consistency: Data consistency is vital for accurate analytics. Checks are conducted to identify and rectify any inconsistencies or anomalies within the datasets, maintaining a coherent and reliable dataset.\n",
      "Timeliness: Timely updates are crucial for reflecting real-time changes in customer profiles. The frequency of data updates aligns with the dynamic nature of customer interactions, ensuring that the information remains current.\n",
      "Frequency of Quality Checks: Regular data quality checks are integrated into the operational workflow of the Customer 360 Data Product. The frequency of these checks is determined by several factors:\n",
      "Source Data Updates: Quality checks align with the update frequency of source data. For datasets with more dynamic sources, such as real-time transactions, checks may be more frequent.\n",
      "Business Needs: The frequency is also tailored to meet specific business requirements and decision-making cycles. Critical datasets may undergo more frequent assessments to support timely and accurate decision-making.\n",
      "Data Volume and Complexity: Larger datasets or those with intricate relationships may require more frequent checks to maintain a high level of data quality.\n",
      "By incorporating these quality metrics and aligning checks with relevant factors, the Customer 360 Data Product ensures that the information driving business decisions remains accurate, reliable, and up-to-date.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Update Frequency:\n",
      "\n",
      "\n",
      "Customer Dataset:\n",
      "Frequency: Daily\n",
      "Explanation: Customer profiles are dynamic and subject to frequent changes. Daily updates ensure that the system reflects the most recent customer interactions, preferences, and behaviours.\n",
      "Product Dataset:\n",
      "Frequency: Weekly\n",
      "Explanation: Product information, including inventory levels, features, and pricing, may not change as rapidly as customer profiles. Weekly updates strike a balance between maintaining accuracy and minimizing unnecessary data processing.\n",
      "Orders Dataset:\n",
      "Frequency: Real-time\n",
      "Explanation: Given the critical nature of order data, updates occur in real-time to capture transactions as they happen. This enables prompt responses to changes in customer behaviour and facilitates accurate order fulfilment.\n",
      "Transactions Dataset:\n",
      "Frequency: Daily\n",
      "Explanation: Daily updates ensure that transactional data, including payment information and order details, are current. This frequency aligns with the need for up-to-date insights into customer purchasing behaviour.\n",
      "Order Line Item Dataset:\n",
      "Frequency: Real-time\n",
      "Explanation: Real-time updates are crucial for granular details related to order line items. This ensures that any changes or modifications are immediately reflected in the dataset.\n",
      "Web Analytics Dataset:\n",
      "Frequency: Hourly\n",
      "Explanantion: Web analytics data can offer insights into customer interactions in near real-time. Hourly updates allow for quick adaptation to changing online behaviours and trends.\n",
      "Brands Dataset:\n",
      "Frequency: Weekly\n",
      "Explanation: Brand-related information, such as partnerships or product launches, may not change as frequently. Weekly updates strike a balance between accuracy and computational efficiency.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Questioning the data \n",
      "What is a data product  Clear definition\n",
      "What is customer 360\n",
      "Interoperability, etc. \n",
      "Interoperability  how c360 is interoperable\n",
      "Govern...\n",
      "What are the datasets in there\n",
      "How are they useful.\n",
      "    Jobys points\n",
      "Where does the data come from? Some brief on the diverse sources\n",
      "Reliability of the data and accuracy\n",
      "Addon in the target audiences.\n",
      "In key feature & capabilities In 5 segmentation and targeting we should specify the criteria.\n",
      "In data volume and update frequency \n",
      "How regularly updates in the data any specific time span?\n",
      "Minimum and maximum data volume?\n",
      "Max data which server can handle\n",
      "Criteria which we are using for the most active and current customers and the filter we are using.\n",
      "In security and compliance consideration\n",
      "Network security\n",
      " Third party data source risk management \n",
      "Backup plan elaborated if needed\n",
      "In assumption and limitation in limitation data constrains what are the types of data limitations we will be going to face.\n",
      "In Business context and application we can add Marketing Optimisation\n",
      "In advanced analytics scenario for Datascientist we can include customer service enhancement by reviewing  feedback.\n",
      "Note : Lens Metrics and dimensions and TrinoSQL  needs to complete\n",
      "\n",
      "Sections to add\n",
      "What is a data product  Clear definition\n",
      "Interoperability, Reliability, Governance, Discoverability\n",
      "C360 - definition\n",
      "Data Sources \n",
      "Data Schema <TABLE , source, dataset, format, update frequency, volume>\n",
      "What are the quality metrics , Frequency\n",
      "Now in the customer 360 Lens has 53 columns , they are, facts about that, definitions, basic EDA\n",
      "Security and Compliance \n",
      "How to query from the lens  Trino \n",
      "Example Queries\n",
      "Sample Results\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def extract_topic_info_from_docx(file_path, topic):\n",
    "    doc = docx.Document(file_path)\n",
    "    topic_info = []\n",
    "    found_topic = False\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if topic.lower() in paragraph.text.lower():\n",
    "            found_topic = True\n",
    "        if found_topic:\n",
    "            topic_info.append(paragraph.text)\n",
    "\n",
    "    return '\\n'.join(topic_info)\n",
    "\n",
    "file_path = 'doc.docx'\n",
    "topic = \"Purpose and Objectives\"\n",
    "topic_info = extract_topic_info_from_docx(file_path, topic)\n",
    "\n",
    "if topic_info:\n",
    "    print(f\"Text content of '{topic}':\\n{topic_info}\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeeac551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content of 'Recency Frequency and Monitoring (RMF) Analysis:':\n",
      "The integration of RFM (Recency, Frequency, Monetary Value) analysis within Customer 360 (C360) lenses has empowered businesses to deeply understand customer behaviour and tailor marketing strategies effectively. By analyzing recent interactions (Recency), frequency of engagement (Frequency), and monetary contributions (Monetary Value), C360 segments customers into groups. This segmentation helps identify high-value, loyal customers for personalized engagement strategies. It also highlights dormant segments requiring re-engagement tactics to prevent potential churn. Utilizing RFM within C360 lenses elevates targeted marketing, enhances customer experiences, and fosters long-term loyalty, driving revenue growth and informed decision-making.\n",
      "Recency: Recency in RFM analysis within C360 lenses illuminates recent customer engagements, indicating their current activity levels and potential ongoing interactions. This metric enables businesses to identify active customers more likely to respond to immediate marketing strategies or personalized offers. Leveraging recency insights allows for targeted engagement, potentially boosting immediate sales or eliciting prompt customer responses, thus elevating overall customer satisfaction and engagement levels.\n",
      "Frequency: Frequency analysis in C360 lenses delves into customer engagement consistency, signifying customer loyalty and habitual interactions. This aspect identifies highly engaged customers, forming the foundation for strategies aimed at retaining loyalty and encouraging repeat interactions. Segmentation and understanding of high-frequency customers empower businesses to design targeted marketing efforts and loyalty programs tailored to maintain and potentially increase these customers' lifetime value, ensuring sustained revenue streams.\n",
      "Monitoring: within C360 lenses' Monetary Value assessment, involves continual tracking of customer transactions and their impact on overall revenue. This process identifies high-value customers whose spending significantly influences business success. Through ongoing monitoring, tailored strategies like exclusive offers or personalized services are applied to retain and increase spending from these customers. Prioritizing enhanced experiences for such clientele maximizes their contribution to the company's financial success.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data Sources and Formats \n",
      " \n",
      "Datasets  \n",
      " Customer  \n",
      "Product  \n",
      "Orders  \n",
      "Transactions  \n",
      "Order line item  \n",
      "Web Analytics  \n",
      "Brands  \n",
      "Entity Relationship Diagram \n",
      "\n",
      "\n",
      " Data Sources   \n",
      "AWS Redshift \n",
      "PostgreSQL \n",
      "Web Analytics (Google)  \n",
      " \n",
      " \n",
      "Data Volume and Update Frequency (Balaji)\n",
      "The volume of data processed by the Customer 360 Data Product depends on the scale and scope of the organization. Factors influencing data volume include the customer base size, transaction frequency, and engagement levels. Update frequencies vary based on the source but are optimized for real-time updates to ensure the most current customer profiles. \n",
      " \n",
      "System Requirements and Dependencies \n",
      " \n",
      "Compute Cluster: Apache Spark \n",
      "Number of Nodes: 3 nodes \n",
      "Node Type: High-memory instances (e.g., r5.4xlarge) with at least 16 CPU cores and 64 GB RAM per node. \n",
      "Total CPU Cores: 48 cores \n",
      "Total RAM: 192 GB \n",
      "Apache Kafka Cluster \n",
      "Number of Brokers: 3 brokers (for redundancy and fault tolerance) \n",
      "Broker Instance Type: Instance type with at least 8 CPU cores and 32 GB RAM per broker. \n",
      "Total CPU Cores: 24 cores \n",
      "Total RAM: 96 GB \n",
      "PostgreSQL Database \n",
      "Instance Type: High-memory instance (e.g., r5.xlarge) with at least 4 CPU cores and 16 GB RAM. \n",
      "Storage: Enough storage capacity to handle the \"orders_enriched_data\" dataset. \n",
      " \n",
      " \n",
      "Data Flow and Architecture Diagram \n",
      " \n",
      "Security and Compliance Considerations \n",
      "Security Measures \n",
      "Encryption: Data encryption in transit and at rest to ensure the confidentiality and integrity of customer information. \n",
      "Access Control: Role-based access control mechanisms to restrict data access based on user roles. \n",
      "Audit Trails: Logging and monitoring features to track user activities and changes to customer profiles. \n",
      "Compliance Considerations \n",
      "Data Privacy Regulations: Adherence to data protection regulations such as GDPR, HIPAA, or industry-specific compliance standards. \n",
      "Data Retention Policies: Implementation of policies for the secure storage and responsible disposal of customer data. \n",
      " \n",
      " \n",
      "Built Using \n",
      " \n",
      "Python: For data processing, transformation, and integration tasks. \n",
      "Apache Kafka: To enable real-time streaming of customer order events. \n",
      "Apache Spark: For handling large-scale data processing efficiently. \n",
      "PostgreSQL: As the primary database to store the enriched order data.  \n",
      " \n",
      "Data Models and Algorithms \n",
      "Description of Data Models Used \n",
      "The Customer 360 Data Product employs a sophisticated blend of data models to construct a comprehensive and unified representation of customer profiles: \n",
      " \n",
      "1. Entity-Relationship Model: \n",
      "Illustrates the relationships and connections between entities, such as customers, transactions, and interactions. \n",
      "2. Graph Model: \n",
      "Captures the intricate connections and dependencies between various elements, providing a visual map of customer relationships and network structures. \n",
      "3. Predictive Models: \n",
      "Harnesses machine learning algorithms, including regression analysis, decision trees, and neural networks, to anticipate customer behaviours and uncover patterns. \n",
      "4. Segmentation Models: \n",
      "Utilizes algorithms like k-means clustering to categorize customers based on shared characteristics, enabling targeted marketing and personalized engagement. \n",
      " \n",
      " \n",
      "Algorithms and Analytical Methods \n",
      "1. Predictive Analytics: \n",
      "Applies advanced algorithms to predict customer behaviours, preferences, and trends, enabling proactive decision-making. \n",
      "Predictive Sales \n",
      "Predictive Churn \n",
      "2. Clustering Algorithms: \n",
      "Utilizes k-means clustering and other algorithms to identify patterns and group customers based on similarities, facilitating tailored strategies. \n",
      " \n",
      "3. Recommendation Algorithms: \n",
      "Implements collaborative filtering and content-based recommendation algorithms to suggest personalized products and services. \n",
      "Market Basket Analysis Based product recommendation \n",
      "Marketing Channel Optimisation and recommendation \n",
      "4. Descriptive Analytics: \n",
      "Leverages statistical methods and analytics tools to derive meaningful insights from historical customer data, aiding in comprehensive understanding. \n",
      " \n",
      " \n",
      "Assumptions and Limitations \n",
      "Assumptions \n",
      "Quality of Input Data: Assumes that the accuracy and reliability of models depend on the quality and completeness of the input data. \n",
      " \n",
      "Predictability: Assumes a degree of predictability in customer behavior based on historical patterns. \n",
      " \n",
      "Limitations \n",
      "Probabilistic Nature: Acknowledges that predictive models are inherently probabilistic and may not always accurately forecast customer behaviors. \n",
      " \n",
      "Data Constraints: Recognizes that data models may not capture all nuances of customer interactions or external factors due to data limitations. \n",
      " \n",
      "Model Validation and Performance Metrics \n",
      "Model Validation \n",
      "Cross-Validation: Utilizes techniques like k-fold cross-validation to assess model performance across different subsets of the dataset. \n",
      " \n",
      "Holdout Validation: Reserves a portion of the dataset for validation to evaluate the model's ability to generalize to new data. \n",
      " \n",
      "Performance Metrics \n",
      "Accuracy: Measures the overall correctness of predictions. \n",
      " \n",
      "Precision and Recall: Evaluates the model's ability to correctly identify positive instances and avoid false positives. \n",
      " \n",
      "F1 Score: Strikes a balance between precision and recall, providing a holistic performance metric. \n",
      " \n",
      "Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC): Assesses the trade-off between sensitivity and specificity, offering insights into the model's discriminatory power. \n",
      " \n",
      "This section unveils the intricacies of the data models and algorithms powering the Customer 360 Data Product, shedding light on the methods employed to create a unified customer perspective, predict behaviours, and segment \n",
      " \n",
      "User Guide \n",
      " \n",
      "Accessing the Data Product \n",
      " \n",
      "The data product can be accessed from the Data Product hub on the corresponding DataOS instance.  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Advanced Features (for Data Scientists) \n",
      " \n",
      "Data Scientists can enjoy majority of their EDA through METIS and Odin which has all the major Univariate and Multivariate analysis in place.  \n",
      " \n",
      "Troubleshooting and Support \n",
      " \n",
      "If they need any support on the data product, they can reach out to Balaji at balaji.ext@tmdc.io, Aravind at aravind@tmdc.io \n",
      " \n",
      " \n",
      "Data Quality and Governance \n",
      " \n",
      "Data Quality Measures and Standards \n",
      "Data quality in the Customer 360 Data Product will be rigorously measured through a multi-faceted approach encompassing various dimensions. Firstly, accuracy checks will be conducted to ensure that the information within customer profiles aligns with the actual interactions and transactions recorded across different sources. Completeness assessments will verify the presence of essential data points, promoting a holistic customer view. Timeliness will be monitored to ensure that customer profiles reflect real-time updates, allowing for timely and relevant insights. Consistency checks will be employed to identify any discrepancies or anomalies in data patterns, while uniqueness measures will ensure the absence of duplicate records. Additionally, data quality audits will be conducted regularly to address any discrepancies, and feedback loops will be established to continuously enhance data accuracy and reliability. Through these comprehensive measures, the Customer 360 Data Product aims to maintain a high standard of data quality, fostering trust in the information provided for strategic decision-making and operational efficiency. \n",
      " \n",
      "Data Cleansing and Preprocessing Steps \n",
      " \n",
      "Data cleaning and preprocessing in the Customer 360 Data Product involve a meticulous process to ensure the quality and integrity of the information. Initially, raw data from diverse sources undergoes cleansing procedures, addressing missing values, outliers, and inconsistencies. Standardization and normalization techniques are applied to ensure uniformity, while duplicate records are identified and eliminated. Data preprocessing involves transforming and aggregating information to create a cohesive dataset. Advanced algorithms are employed to handle complex tasks such as imputing missing values and encoding categorical variables. Through this thorough cleaning and preprocessing pipeline, the Customer 360 Data Product guarantees a reliable foundation for robust analytics and actionable insights. \n",
      " \n",
      "\n",
      "Data Governance Policies and Data Stewardship\n",
      "There are two types of Data Governance policies:\n",
      "Role-Based Access Control (RBAC):\n",
      "Users see only corresponding rows and datasets based on their access role.\n",
      "Discretionary Access Control (DAC):\n",
      "Currently, DAC (full access) is granted to:\n",
      "For RBAC:\n",
      "Certain Personally Identifiable Information (PII) is masked for each category.\n",
      "Data Steward Responsibilities:\n",
      "Clearly define roles and responsibilities for data ownership.\n",
      "Appoint data stewards responsible for overseeing data quality and integrity.\n",
      "\n",
      " \n",
      "Data Lineage and History Tracking \n",
      " \n",
      "In DataOS, the data lineage of the Custome360 product can be accessed through the Hera application within DataOS. The change history and history tracking is possible through METIS.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Customer 360 Lens \n",
      " \n",
      "The customer 360 Lens is built by connecting the entities mentioned above as shown in the diagram below,  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "The definition of the lens can be found in the following URL \n",
      "https://github.com/StacknexusDatanators/retail-accelerator/blob/main/dataos_yamls/lens/c360_solution_accelerator.yaml \n",
      " \n",
      " \n",
      "Integration and Deployment \n",
      " \n",
      "Integration with Other Systems (for Data Engineers) \n",
      " \n",
      "The data can be integrated with any other data system through various methods,  \n",
      " \n",
      "Spark based Flare jobs where the raw datasets can be integrated or migrated to their destination \n",
      "Flare is a Data Migration Engine Built on top of Apache Spark. A typical Flare YAML has the following Structure,  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "You can find more information about flare in the following link \n",
      "(https://dataos.info/resources/stacks/flare/#syntax-of-flare-yaml-configuration) \n",
      " \n",
      "Here is an example flare job for an enriched orders dataset that can be built from the Customer360 Data Product  \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "You can access the source YAMLs in the following github repository.  \n",
      " \n",
      "https://github.com/StacknexusDatanators/retail-accelerator \n",
      " \n",
      " \n",
      "TrinoSQL based query connection to all the datasets and lenses avaxtilable \n",
      " (NEED TO COMPLETE)\n",
      " \n",
      "Scalability and Performance Considerations \n",
      "Scalability and performance considerations in the Customer 360 Data Product involve optimizing infrastructure to accommodate growing datasets and user demands. Implementing scalable architecture ensures responsiveness, allowing the platform to efficiently handle increased data volume, user interactions, and evolving analytics needs, fostering a seamless and high-performance experience. \n",
      " \n",
      "Backup and Disaster Recovery Procedures \n",
      " \n",
      "Backup and disaster recovery strategies for the Customer 360 Data Product are pivotal for safeguarding against unforeseen events. Regular backups of critical data are conducted to secure against accidental deletions, system failures, or cyber threats. An effective disaster recovery plan outlines procedures for swift data restoration, minimizing downtime. Off-site data storage and redundancy measures are implemented to enhance resilience. Periodic testing of recovery procedures ensures their efficacy, guaranteeing that the Customer 360 Data Product can swiftly recover from disruptions, maintain data integrity, and resume operations, thereby safeguarding against potential data loss or system outages. \n",
      " \n",
      " \n",
      "Use Cases and Scenarios \n",
      " \n",
      "Typical Use Cases for Business Analysis \n",
      " \n",
      "1. Personalized Marketing Campaigns: \n",
      "Analyze customer preferences and behavior to tailor marketing campaigns for increased engagement and conversion rates. \n",
      " \n",
      "2. Customer Segmentation: \n",
      "Utilize segmentation models to categorize customers based on demographics and behaviors, optimizing targeted marketing strategies. \n",
      " \n",
      "3. Churn Prediction and Retention Strategies: \n",
      "Implement predictive analytics to identify at-risk customers and devise proactive retention strategies, reducing churn rates. \n",
      " \n",
      "4. Sales Performance Analysis: \n",
      "Evaluate sales data to identify top-performing products, optimize pricing strategies, and enhance overall sales performance. \n",
      " \n",
      "5. Customer Satisfaction Analysis: \n",
      "Analyze customer feedback and support interactions to assess satisfaction levels and improve customer service strategies. \n",
      " \n",
      "Advanced Analytical Scenarios for Data Scientists \n",
      " \n",
      "1. Predictive Modeling for Revenue Forecasting: \n",
      "Develop and deploy predictive models to forecast revenue based on historical data and market trends. \n",
      " \n",
      "2. Anomaly Detection for Fraud Prevention: \n",
      "Implement advanced anomaly detection algorithms to identify unusual patterns indicative of potential fraud in transactions. \n",
      " \n",
      "3. Natural Language Processing (NLP) for Customer Sentiment Analysis: \n",
      "Apply NLP techniques to analyze customer reviews and social media data for sentiment analysis, informing brand perception strategies. \n",
      " \n",
      "4. Machine Learning Recommender Systems: \n",
      "Develop personalized recommender systems using machine learning algorithms for accurate product or content recommendations. \n",
      " \n",
      "5. Time Series Analysis for Demand Forecasting: \n",
      "Utilize time series analysis to forecast customer demand, optimizing inventory management and supply chain operations. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What is a Data Product?\n",
      "A data product is a package of data, metadata, characteristics of the data, means of use, Governance, Quality. \n",
      "Data Products Spec sheet:\n",
      "A data product spec sheet will also include the usecases, scenarios and other information..\n",
      "Data product spec sheet encapsulates the entirety of information that users seek and consume about the product, including exhaustive details on its sources, technologies employed, use cases, and generated insights. This encompasses all relevant data-driven insights, aims, and functionalities, serving as a holistic repository of knowledge vital for understanding the product's utility from a business perspective. Data product spec sheet details use cases such as: spanning software applications, platforms, algorithms, or systems, serve to transform raw data into actionable insights, optimizing processes, enhancing user experiences, and fostering informed decision-making across diverse industries and domains.\n",
      "Data Product platform:\n",
      "It supports ingress data interoperability and egress data interoperability. \n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def extract_topic_info_from_docx(file_path, topic):\n",
    "    doc = docx.Document(file_path)\n",
    "    topic_info = []\n",
    "    found_topic = False\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if topic.lower() in paragraph.text.lower():\n",
    "            found_topic = True\n",
    "        elif found_topic:\n",
    "            if paragraph.style.name.startswith('Heading'):\n",
    "                break\n",
    "            topic_info.append(paragraph.text)\n",
    "\n",
    "    return '\\n'.join(topic_info)\n",
    "\n",
    "file_path = 'doc.docx'\n",
    "topic = \"Recency Frequency and Monitoring (RMF) Analysis:\"\n",
    "topic_info = extract_topic_info_from_docx(file_path, topic)\n",
    "\n",
    "if topic_info:\n",
    "    print(f\"Text content of '{topic}':\\n{topic_info}\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba80dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'Customer 360 Lens' not found in the document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/2020514318.py:7: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  topic_element = soup.find('strong', text=topic)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_topic_info_from_html(file_path, topic):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        topic_element = soup.find('strong', text=topic)\n",
    "        if topic_element:\n",
    "            next_sibling = topic_element.find_next_sibling()\n",
    "            if next_sibling:\n",
    "                topic_info = next_sibling.get_text().strip()\n",
    "                return topic_info\n",
    "        return None\n",
    "\n",
    "file_path = 'doc.html'\n",
    "topic = \"Customer 360 Lens\"\n",
    "topic_info = extract_topic_info_from_html(file_path, topic)\n",
    "\n",
    "if topic_info:\n",
    "    print(f\"Text content of '{topic}':\\n{topic_info}\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44255263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'Target Audience' not found in the document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/2342164223.py:7: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  topic_element = soup.find('strong', text=topic)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_topic_info_from_html(file_path, topic):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        topic_element = soup.find('strong', text=topic)\n",
    "        if topic_element:\n",
    "            next_sibling = topic_element.find_next_sibling()\n",
    "            if next_sibling and hasattr(next_sibling, 'get_text'):\n",
    "                topic_info = next_sibling.get_text().strip()\n",
    "                return topic_info\n",
    "        return None\n",
    "\n",
    "file_path = 'doc.html'\n",
    "topic = \"Target Audience\"\n",
    "topic_info = extract_topic_info_from_html(file_path, topic)\n",
    "\n",
    "if topic_info:\n",
    "    print(f\"Text content of '{topic}':\\n{topic_info}\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bbaab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'Target Audience' not found in the document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/4174827926.py:9: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  topic_element = soup.find('strong', text=topic)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_path = 'doc.html' \n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "def extract_topic_info_from_html(html_content, topic):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    topic_element = soup.find('strong', text=topic)\n",
    "    if topic_element:\n",
    "        topic_info = ''\n",
    "        for elem in topic_element.find_next_siblings():\n",
    "            if elem.name and elem.name.startswith('h'):\n",
    "                break\n",
    "            topic_info += elem.get_text(strip=True) + ' '\n",
    "        return topic_info\n",
    "    return None\n",
    "\n",
    "topic = \"Target Audience\"\n",
    "topic_info = extract_topic_info_from_html(html_content, topic)\n",
    "\n",
    "if topic_info:\n",
    "    print(f\"Text content of '{topic}':\\n{topic_info}\")\n",
    "else:\n",
    "    print(f\"Topic '{topic}' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f565e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51017/3001307022.py:11: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  target_audience_section = soup.find('strong', text='Target Audience')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# If the section is found, extract the text content of the next sibling element\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_audience_section:\n\u001b[0;32m---> 15\u001b[0m     target_audience_info \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_audience_section\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_next_sibling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText content of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget Audience\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtarget_audience_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML file\n",
    "with open('doc.html', 'r') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Parse the HTML content with Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the \"Target Audience\" section\n",
    "target_audience_section = soup.find('strong', text='Target Audience')\n",
    "\n",
    "# If the section is found, extract the text content of the next sibling element\n",
    "if target_audience_section:\n",
    "    target_audience_info = target_audience_section.find_next_sibling().get_text().strip()\n",
    "    print(f\"Text content of 'Target Audience':\\n{target_audience_info}\")\n",
    "else:\n",
    "    print(\"Topic 'Target Audience' not found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a402e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
